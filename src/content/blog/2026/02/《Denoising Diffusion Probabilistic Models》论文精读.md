---
title: "《Denoising Diffusion Probabilistic Models》论文精读"
categories: Papers
tags: ['深度学习','图像生成','扩散模型']
id: "papers-DDPM"
date: 2026-02-25 17:32:14
cover: "https://zycs-img-2lg.pages.dev/v2/aKR00D4.png"
---

:::note{type="success"}

本文是论文精读系列的第一篇，论文《Denoising Diffusion Probabilistic Models》是图像生成领域中扩散模型的开山之作，也是目前常用的生成模型的根基。本文旨在从具备一定高数、线代、概率论基础，对深度学习基础模型有较多了解的本科生角度出发，全面解读这篇论文，以求对这篇论文的思想、数学原理、模型架构有较为深刻的了解。后续也会继续更新图像生成领域的论文精读和代码学习，欢迎关注。

:::

# 前置数学知识

## 高斯分布的运算法则与重参数化技巧

### 独立高斯分布的线性组合

假设有两个相互独立的随机变量都服从高斯分布：

$X \sim \mathcal{N}(\mu_1, \sigma_1^2)$ 和 $Y \sim \mathcal{N}(\mu_2,\sigma_2^2)$

则对于线性组合起来的新变量 $Z=aX+bY$ (其中 $a, b$ 是常数)，则 Z 依然服从高斯分布，其均值和方差遵循以下规则：

$$ Z \sim \mathcal{N}(a \mu_1 + b \mu_2,a^2 \sigma_1^2 + b^2 \sigma_2^2) $$

**在 DDPM 中的作用**：前向扩散过程就是不断往图像中按比例加独立的高斯噪音。

### 重参数化技巧

在深度学习里，直接从一个带有参数 $\mu$ 和 $\sigma$ 的分布 $\mathcal{N}(\mu,\sigma)$ 中采样一个数据点 $x$ 是不可导的，梯度无法反向传播。

因此，将采样拆为：

1. 首先，从标准正态分布中采样一个基础噪声: $\epsilon \sim \mathcal{N}(0, 1)$
2. 然后，通过线性变换得到我们想要的 $x: x=\mu_{\theta}+\sigma_{\theta} \cdot \epsilon$
3. 这样一来， $x$ 对 $\mu_{\theta}$ 和 $\sigma_{\theta}$ 的关系就变成了完全确定的数学函数，梯度就可以正常传播：
   - $x$ 对 $\mu_{\theta}$ 的偏导数是 1。
   - $x$ 对 $\sigma_{\theta}$ 的偏导数是 $\epsilon$。
1. 这样梯度就能通过计算图，神经网络的权重 $\theta$ 就能顺利得到更新了。

## 马尔可夫链

定义：马尔可夫链描述了一系列状态的演变，具有“无记忆性”假设——系统的下一个状态只有当前状态有关，而与过去的任何状态都无关。即：

$$ P(x_t|x_{t-1},x_{t-2},...,x_0)=P(x_t|x_{t-1}) $$

**在 DDPM 中的作用**：

- 前向过程：论文将逐渐加噪的过程定义为马尔可夫链 $q(x_{1:T}|x_0) := \prod_{t=1}^T q(x_t|x_{t-1})$。即第 $t$ 步的加噪图像 $x_t$ 只依赖于第 $t-1$ 步的图像 $x_{t-1}$
- 反向过程：模型要学习的去噪过程也  是一个马尔可夫链 $p_{\theta}(x_{0:T}):=p(x_T)\prod_{t=1}^Tp_{\theta}(x_{t-1}|x_t)$。即去噪恢复出 $x_{t-1}$ 只依赖于当前的 $x_t$。

> 数学符号解读：
>
> -  关于 $1:T$
>
> 表示从 1 到 $T$ 的一个连续的序列。例如 $x_{1:T}$ 就是 $x_1,x_2,x_3,...,x_T$ 这一连串变量的集合缩写
>
> -  关于 $:=$
>
> 这个符号读作“定义为”，表示“我此刻人为规定左边的符号等于右边的内容”。 

## KL散度

KL 散度的通用公式：

对于两个连续的概率分布 P 和 Q，衡量 $P$ 和 $Q$ 差异的 KL 散度定义为一个积分公式：

$$ D_{KL}(P || Q) = \int_{-\infty}^{\infty} p(x) \log \left( \frac{p(x)}{q(x)} \right) dx $$

我们要让神经网络去模仿真实的去噪过程，我们就需要一把尺子来衡量网络当前的模仿和真实的去噪过程之间的差异，这把尺子就是 KL 散度。

**在 DDPM 中的关键作用**：数学上有一个性质：计算两个高斯分布之间的 KL 散度，是有确定的闭式解的。

> 闭式解：在数学建模和优化中，如果一个复杂的方程或积分能够通过有限步的基本数学运算（加减乘除、指数、对数等）直接写出一个精确的代数表达式，我们就说它有“闭式解”或“解析解”。

两个单变量高斯分布 $P \sim \mathcal{N}(\mu_1, \sigma_1^2)$ 和 $Q \sim \mathcal{N}(\mu_2, \sigma_2^2)$ 之间的 KL 散度积分可以直接简化为一个纯代数公式：

$$ D_{KL}(P||Q)=\log \left(\frac{\sigma_2}{\sigma_1} \right) + \frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2} - \frac{1}{2} $$

积分号消失了。这意味着在构造损失函数时，我们只需要提取出预测分布和真实分布的均值和方差，代入这个简单的算术公式即可。论文中提到，所有的 KL 散度都是高斯分布之间的比较，因此可以用闭式解直接计算。

## 极大似然估计与变分推断

### 极大似然估计（MLE）

假设真实数据集分布是 $q(x_0)$，我们的模型分布是 $p_\theta(x_0)$。MLE 的核心思想是：调整模型参数 $\theta$，使得真实数据在我们模型中出现的概率最大化。

目标函数（取对数方便求导）： $\max_\theta \mathbb{E}_{q(x_0)}[\log p_\theta(x_0)]$。

在机器学习中，我们通常把最大化问题变成最小化问题，即最小化负对数似然： $\min_\theta \mathbb{E}[-\log p_\theta(x_0)]$

> $\mathbb{E}_{q(x_0)}[\log p_\theta(x_0)]$:
>
> - $q(x_0)$：代表的是真实世界（也就是训练集）的客观数据分布。
> - 假设训练集里有 $N$ 张真实的图片，分别记为 $x^{(1)}, x^{(2)}, \dots, x^{(N)}$。
> - 连乘：模型认为这 $N$ 张真实图片同时存在的总概率，是它们各自概率的连乘： $p_\theta(x^{(1)}) \cdot p_\theta(x^{(2)}) \dots p_\theta(x^{(N)})$
> - 取对数：因为连乘很容易导致数值下溢，我们通常取对数，把连乘变成连加 $\sum_{i=1}^N \log p_\theta(x^{(i)})$
> - 求平均：为了让损失函数不受数据集大小 $N$ 的影响，我们通常除以 $N$，求个平均值： $\frac{1}{N} \sum_{i=1}^N \log p_\theta(x^{(i)})$
> - 所以， $\mathbb{E}_{q(x_0)}[\log p_\theta(x_0)]$ 的意思就是：“从真实训练集 $q(x_0)$ 里抓取一张图片 $x_0$，喂给模型，让模型算出一个对数概率 $\log p_\theta(x_0)$，然后把所有真实图片算出来的这个分值求个平均。”

### 变分推断

**问题**：在扩散模型中， $p_\theta(x_0)$ 实际上是由纯噪声 $x_T$ 经过 $T$ 步降噪得来的。要计算出 $p_\theta(x_0)$，必须把所有可能的中间状态 $x_{1:T}$ 全部积分掉： $p_\theta(x_0) = \int p_\theta(x_{0:T}) dx_{1:T}$。这个积分在千万维的图像空间是根本算不出来的。

> $p_\theta(x_0) = \int p_\theta(x_{0:T}) dx_{1:T}$:
>
> - 下标 $\theta$：
>
>   在深度学习的数学表达里， $\theta$ 永远代表神经网络中所有可学习的权重参数。
>
>   - 如果不带 $\theta$ 的 $p$ 或 $q$，通常指代自然界固有的规律，或者我们人为写死的、不需要训练的规则（比如前向加噪过程）
>   - 带着 $\theta$ 的 $p_\theta$，意思是“这个概率是由我正在训练的神经网络算出来的”。我们要不断调整网络参数 $\theta$，来改变这个概率分布的形状
>
> - $x_0$：代表最终的真实观测数据
> - $x_{1:T}$：代表一系列的潜变量序列，即 $x_1, x_2, \dots, x_T$
> - $p_\theta(x_{0:T})$：代表观测数据 $x_0$ 和所有潜变量 $x_{1:T}$ 的联合概率分布[[《Denoising Diffusion Probabilistic Models》论文精读#马尔可夫链]]
> - $\int \dots dx_{1:T}$：这是一个高阶多重积分，将这个符号完全展开：
>
>  $$ \int \dots dx_{1:T} \equiv \underbrace{\int \int \dots \int}_{T \text{次}} \dots dx_1 dx_2 \dots dx_T $$
>
>   在概率论中，这个积分操作对应着求边缘概率密度——假设我们有一个联合概率密度函数 $f(x, y)$，如果我们只想知道 $x$ 的概率分布，而不在乎 $y$ 取什么值，数学上的做法是对 $y$ 积分： $f(x) = \int f(x, y) dy$。对应到这个积分操作中:
>
>  - 被积函数是联合分布 $p_\theta(x_0, x_1, \dots, x_T)$。
>  - $\int \dots dx_{1:T}$ 的数学动作，就是将联合概率密度函数对所有的潜变量 $x_1$ 到 $x_T$ 进行积分。
>  - 通过这种操作，我们把一个定义在 $\mathbb{R}^{D \times (T+1)}$ 空间上的联合分布，投影到了只包含 $x_0$ 的 $\mathbb{R}^D$ 空间上，从而得到边缘分布 $p_\theta(x_0)$。

**思想**：既然直接算算不出来，那么可以找一下界——找到一个一定比 $\log p_\theta(x_0)$ 小，但是可以被计算的公式，这个公式就是 ELBO（证据下界），则当我们最大化这个下界时，相当于简洁地最大化了目标概率。

**在 DDPM 中的应用**：利用 Jenson 不等式，得到了以下 ELBO：

$$ \mathbb{E}[-\log p_\theta(x_0)] \le \mathbb{E}_q \left[ -\log p(x_T) - \sum_{t \ge 1} \log \frac{p_\theta(x_{t-1}|x_t)}{q(x_t|x_{t-1})} \right] =: L $$

这样就把复杂的积分问题，转化成了优化期望值的问题。

---

# 剖析扩散过程

## 单步加噪

论文在公式 (2) 中定义了单步的加噪过程：

$$ q(x_t|x_{t-1}) := \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I) $$

**公式解读**：

- $q(x_t|x_{t-1})$：代表条件概率密度函数。该表达式描述了：在给定前一个状态 $x_{t-1}$ 完全已知的条件下，当前状态 $x_t$ 的概率分布情况。因为 $x_t$ 只依赖于紧邻的前一步 $x_{t-1}$，这体现了严格的马尔可夫性质。

  在深度学习中，我们通常用 $q$ 来表示前向的、用来近似或人为设定的概率分布，而用 $p$ 来表示模型真正要去学习和生成的概率分布。所以在这里， $q$ 专门用来指代“前向加噪过程”的分布。

> $q(x_T|x_{t-1})$ 与 $q(x_T)$ 的关系(全概率公式)：
>
> $$ q(x_T) = \int q(x_T|x_{t-1}) q(x_{t-1}) dx_{t-1} $$

- $\mathcal{N}$：作者人为规定每一次加噪都必须加上高斯噪音。之所以这么选，是因为高斯分布的加法法则非常完美[[《Denoising Diffusion Probabilistic Models》论文精读#独立高斯分布的线性组合]]。

> 数学符号解读：
>
> - $\mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)$：在多维空间的正式数学表达中，高斯分布通常写成 $\mathcal{N}(\text{变量}; \text{均值}, \text{协方差矩阵})$。
>
>   - $x_t$：这是我们要研究的随机变量。分号的意思是“以……为条件”或“参数为……”。即“变量 $x_t$ 是一个服从正态分布的随机变量，它的分布参数如下”。

- $\beta_t$：方差调度。它决定了在第 $t$ 步我们要往图像中注入多少噪声。
  - 它的本质：它是一个介于 0 和 1 之间的数值序列。
  - 在本文中的设定：在论文的实验中（第 4 节），作者把总步数设为 $T=1000$ ，并且将 $\beta_t$ 设定为从 $\beta_1 = 10^{-4}$ 线性增加到 $\beta_T = 0.02$ 的常数。
  - 为什么慢慢变大： $t$ 较小时，原图信息很清晰，我们只加微小的噪声（ $0.0001$ ）；越到后面（ $t$ 接近 1000），图像已经被破坏得差不多了，我们需要加更大的噪声（ $0.02$ ）来确保最终的图像 $x_T$ 彻底变成纯高斯噪声。
- $\sqrt{1-\beta_t}x_{t-1}$ （均值 $\mu$ ）：因为 $\beta_t$ 很小，所以 $\sqrt{1-\beta_t}$ 是一个非常接近 1 但是稍微小于 1 的数。这个设计的巧妙之处在于：我们不是直接在 $x_{t-1}$ 上原封不动地加噪声，而是先把 $x_{t-1}$ 的像素值稍微衰减一点点，向 0 靠拢。
- $\beta_t I$ （方差/协方差矩阵 $\Sigma$ ）：这代表我们要加进去的噪声强度大小。 $\beta_t$ 就是方差。后面跟着的 $I$ 是单位矩阵 。因为我们的图像有很多像素点，这个 $I$ 的数学含义是保证图像上的每一个像素点都在独立地加噪声，互相不干扰。

## 重参数化

> 为了简化表达，论文在公式(4)之前定义了一个新变量： $\alpha_t := 1 - \beta_t$

根据单步加噪公式和新变量 $\alpha_t$，我们可以把采样的过程写成一个确定的等式。假设 $\epsilon_{t-1} \sim \mathcal{N}(0, I)$ 是我们采样的标准高斯噪声：[[《Denoising Diffusion Probabilistic Models》论文精读|DDPM#高斯分布的运算法则与重参数化技巧]]

$$ x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1} $$

## 公式推导

同理，上一步的图像 $x_{t-1}$ 也是由 $x_{t-2}$ 加噪得来的： $$ x_{t-1} = \sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2} $$

现在把这个 $x_{t-1}$ 代入前一步的 $x_{t}$ 等式中：

$$ x_t = \sqrt{\alpha_t} (\sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2}) + \sqrt{1 - \alpha_t} \epsilon_{t-1} $$

展开括号：

$$ x_t = \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + (\sqrt{\alpha_t (1 - \alpha_{t-1})} \epsilon_{t-2} + \sqrt{1 - \alpha_t} \epsilon_{t-1}) $$

再利用[[《Denoising Diffusion Probabilistic Models》论文精读#独立高斯分布的线性组合]]合并噪声：

在这里， $\epsilon_{t-2}$ 和 $\epsilon_{t-1}$ 的方差都是 $1$，所以，这两个噪声合并成一个新的标准高斯噪声（我们叫它 $\bar{\epsilon}$ ），它的系数（也就是新方差的平方根）：

$(\sqrt{\alpha_t (1 - \alpha_{t-1})})^2 + (\sqrt{1 - \alpha_t})^2$

$= \alpha_t(1 - \alpha_{t-1}) + 1 - \alpha_t$

$= \alpha_t - \alpha_t\alpha_{t-1} + 1 - \alpha_t$

$= 1 - \alpha_t\alpha_{t-1}$

噪声合并后，等式变为了： $$ x_t = \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\epsilon} $$

顺着这个规律一直推下去，直到最开始的 $x_0$，最终 $x_t$ 的表达式为： $$ x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon $$

这里 $\bar{\alpha}_t := \prod_{s=1}^t \alpha_s$

## 最终分布

通过上一步的推导，我们便得到了论文里的公式(4)：

$$ q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I) $$

它使得模型可以在任何时间步 $t$ 直接采样出 $x_t$，而不需要一步步遍历。

随着时间步 $t$ 不断变大，因为 $\alpha_t$ 都是小于 1 的数，所以他们的连乘积 $\bar{\alpha}_t$ 会越来越小，最终趋近于 0。

1. 均值 $\sqrt{\bar{\alpha}_t}x_0$ 会慢慢趋近于 0。这意味着原图 $x_0$ 的特征被逐渐抹除了。
2. 方差 $(1 - \bar{\alpha}_t)I$ 会趋近于 $1 \cdot I$。这意味着随机噪声的权重越来越大。

当 $t=T$ （即 1000 步）时， $\bar{\alpha}_T \approx 0$。此时 $x_T$ 的分布就变成了 $\mathcal{N}(0, I)$ 。也就是说，到了最后一步，所有的原始图像信息都灰飞烟灭，剩下的只有完美的、纯粹的标准高斯噪声 。

---

# 拆解反向过程

## 模型定义

在反向过程中，我们要训练一个神经网络，去一步步地消除图像中的噪声。数学上，这个过程被定义为一个由参数 $\theta$ 控制的马尔可夫链：

$$ p_\theta(x_{t-1}|x_t) := \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)) $$

我们的目标是让这个网络输出均值 $\mu_{\theta}$ 和协方差 $\Sigma_\theta$

## 工程简化：固定方差

作者在消融实验中尝试了让神经网络同时去预测对角协方差矩阵，结果发现，学习方差会导致训练极其不稳定，并且生成的图像质量更差。因为均值决定了图像“是什么样”，而方差只决定了分布的“宽窄”，在 DDPM 的设定下，固定方差不仅能简化模型，还能带来更好的生成效果。

于是，作者直接把方差 $\Sigma_\theta(x_t, t)$ 设定为一个未经训练的、与时间相关的常数 $\sigma_t^2 I$。作者给出了两个数学上合理的极端边界选择：

1. 直接让 $\sigma_t^2 = \beta_t$。这个选择在假设原始数据 $x_0$ 也是标准高斯分布时是最佳的。
2. 让 $\sigma_t^2 = \tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$。这个选择在假设原始数据 $x_0$ 只有一个确定点时是最佳的。

论文中提到，实验表明这两种常数设定的结果差不多，都非常好。

## 推导标准答案

利用变分推断，得到我们要最小化的变分上界：

$$ \mathbb{E}[-\log p_\theta(x_0)] \le \mathbb{E}_q \left[ -\log p(x_T) - \sum_{t \ge 1} \log \frac{p_\theta(x_{t-1}|x_t)}{q(x_t|x_{t-1})} \right] =: L $$

为了让这个界限 $L$ 在代码里跑起来，作者进一步利用马尔可夫链和贝叶斯定理，将上式重写为:

$$ \mathbb{E}_q \left[ \underbrace{D_{\text{KL}}(q(\mathbf{x}_T | \mathbf{x}_0) \parallel p(\mathbf{x}_T))}_{L_T} + \sum_{t>1} \underbrace{D_{\text{KL}}(q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 | \mathbf{x}_1)}_{L_0} \right] $$

这个重写把整个模型拆分成了三个部分的KL 散度：

- $L_T$：前向最终状态与纯噪声的散度（常数，不参与训练）
- $L_{t-1}$：真实的去噪步骤 $q(x_{t-1}|x_t, x_0)$ 与神经网络预测的去噪步骤 $p_\theta(x_{t-1}|x_t)$ 之间的 KL 散度 。
- $L_0$ ：最后一步生成图像的重建损失 。

这样，我们便将 $q(x_{t-1}|x_t, x_0)$ 作为我们预测网络的标准答案，它也是一个高斯分布： $$ q(x_{t-1}|x_t, x_0) = \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t I) $$

其中，真正的均值 $\tilde{\mu}_t$ 是由 $x_t$ 和 $x_0$ 计算出来的： $$ \tilde{\mu}_t(x_t, x_0) := \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t $$

方差 $\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$

> 公式推导：
>
> 根据贝叶斯定理： $$ q(x_{t-1}|x_t, x_0) = \frac{q(x_t|x_{t-1}, x_0) q(x_{t-1}|x_0)}{q(x_t|x_0)} $$
>
> - $q(x_t|x_{t-1}, x_0)$：因为马尔可夫链的无记忆性，它其实就等于单步加噪 $q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t}x_{t-1}, \beta_t I)$。    
> - $q(x_{t-1}|x_0)$：利用[[《Denoising Diffusion Probabilistic Models》论文精读#最终分布]]，它是 $\mathcal{N}(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}}x_0, (1-\bar{\alpha}_{t-1})I)$。 
> - $q(x_t|x_0)$：同理，它是 $\mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)I)$ 。
>
> 这三个部件全都是标准的高斯分布。把它们各自的概率密度函数（包含指数函数 $e$ 的一长串公式）代入到贝叶斯公式中相乘和相除，经过一番非常繁琐的代数展开和合并同类项，最终就能精简出论文公式 (6) 和 (7) 里的 $\tilde{\mu}_t$ 和 $\tilde{\beta}_t$ 。

---

# 推导简化目标函数

## 从KL散度到均方误差

我们希望网络预测的均值 $\mu_\theta$ 尽可能接近真正的均值 $\tilde{\mu}_t$。因为这两个分布都是高斯分布，它们之间的 KL 散度（也就是前一步的 $L_{t-1}$ 项）可以直接用闭式解[[《Denoising Diffusion Probabilistic Models》论文精读#KL散度]]算出来，结果就是一个均分误差（MSE）公式：

$$ L_{t-1} = \mathbb{E}_q \left[ \frac{1}{2\sigma_t^2} ||\tilde{\mu}_t(x_t, x_0) - \mu_\theta(x_t, t)||^2 \right] + C $$

这里的 $C$ 是一个跟我们要训练的参数 $\theta$ 无关的常数，可以直接忽略。

## 消灭 $x_0$

在实际生成图片的时候，我们手里只有纯噪声 $x_T$，我们是没有上帝视角 $x_0$ 的。既然如此，我们便需要把 $\tilde{\mu}_t$ 公式里的 $x_0$ 给替换掉。

由[[《Denoising Diffusion Probabilistic Models》论文精读#公式推导]]最后的公式可知：

$$ x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon $$

稍微变形，便可用 $x_t$ 和 $\epsilon$ 把 $x_0$ 反向表达出来：

$$ x_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} (x_t - \sqrt{1 - \bar{\alpha}_t}\epsilon) $$

现在，把这个 $x_0$ 代入 $\tilde{\mu}_t$ 的公式中去，化简可得：

$$ \tilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon \right) $$

## 范式转化

在化简之后的 $\tilde{\mu}_t$ 公式里， $x_t$ 是当前这步模糊的图像，是作为输入喂给神经网络的已知量； $\alpha_t$ 和 $\beta_t$ 都是我们提前设定好的常数（已知量）。则在这个公式里，唯一未知的变量，就是那个纯高斯噪音 $\epsilon$ 。

既然如此，我们便可以定义神经网络的输出形式为：

$$ \mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t, t) \right) $$

即，我们让神经网络扮演一个“噪声预测器” $\epsilon_{\theta}$ 。

## 最终损失函数

最后，再把这个重新定义的 $\mu_{\theta}$ 和消去 $x_0$ 的 $\tilde{\mu}_t$ 代回到损失函数 $L_{t-1}$ 中，我们得到下式：

$$\mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \frac{\beta_t^2}{2\sigma_t^2 \alpha_t (1 - \bar{\alpha}_t)} \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}, t) \right\|^2 \right]$$

再把前边的权重系数去掉（作者发现去掉系数、用简化的无权重版本效果反而更好），便得到了最终的损失函数：

$$ L_{simple}(\theta) := \mathbb{E}_{t, x_0, \epsilon} \left[ ||\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t)||^2 \right] $$

总结 $L_{simple}$ 的执行过程：

1. 从训练集随便抽出一张真图 $x_0$ 。
2. 随便生成一个高斯噪声 $\epsilon$ 。
3. 随便抽一个时间步 $t$ 。
4. 把 $x_0$ 和 $\epsilon$ 混合，算出加噪后的图像 $x_t$ 。
5. 把 $x_t$ 喂给神经网络，让它预测出一个噪声 $\epsilon_\theta$ 。
6. 计算真实的 $\epsilon$ 和网络预测的 $\epsilon_\theta$ 之间的均方误差，然后反向传播更新参数。

----

# 探究架构细节

## 核心骨架：基于 Wide ResNet 的 U-Net

为了表示反向过程，作者使用了一个 U-Net 骨架。

### U-Net

- 任务：输入一张图片，输出一张空间分辨率（宽和高）完全一样的图片。在 DDPM 中，输入是 $3 \times 32 \times 32$ 的带噪图像 $x_t$ ，输出是 $3 \times 32 \times 32$ 的预测噪声 $\epsilon_\theta$ 。
- 编码器/下采样：负责压缩。图像经过一层层的卷积和池化（或步长卷积），特征图的长宽越来越小，但通道数越来越深。这个过程网络在提取高级语义信息。
- 瓶颈层：特征图被压缩到最下，这里浓缩了整张图片的全局信息。
- 解码器/上采样：负责放大。用最近邻插值+标准卷积，将特征图一步步放大回原来的分辨率。

> - 最近邻插值：最无脑的放大。如果要把$4 \times 4$ 放大到$8 \times 8$ ，就把原来的每个像素直接复制成 $2 \times 2$ 的四个一模一样的像素，这个操作没有任何可学习的参数。
> - 标准卷积：插值放大后，图像会全是马赛克。紧接着立刻上一个 $3 \times 3$ 的标准卷积层。这个卷积层包含可学习的参数，它的任务就是把输入的马赛克变得平滑，提取出合理的特征。

- 跳跃连接：如果只靠解码器放大，图像的边缘和细节早就丢失了。U-Net 会把编码器里相同分辨率的特征图，直接拼接到解码器的特征图上。

> 这里的拼接是在通道维度进行拼接。
>
> 假设在 U-Net 的编码器，某一层输出的特征图形状是 $(B,C,H,W)$ ，当这个特征图通过跳跃连接传到解码器对应的层时，解码器也有一个经过上采样得到的特征图，形状也是 $(B,C,H,W)$  。U-Net 会把这两个张量沿着通道维度拼接起来，形状变为 $(B,2C,H,W)$ 。随后，U-Net 会紧接着跟上一个标准的卷积层，这个卷积层的任务就是把这个 $2C$ 个通道的信息进行融合，然后再把通道数压回$C$ ，继续往上层传递。

## 时间条件注入

- 核心目的：我们的网络要在不同的时间步 $t$ 执行不同强度的去噪任务，但在所有的时间步里，网络参数是完全共享的。所以必须让网络在每次前向传播时知道当前处于去噪过程的哪一步 $t$ 。
- 编码方式：作者借鉴了transformer 模型中的正弦位置编码公式，利用不同频率的正弦和余弦函数，为每一个时间步 $t$ 生成一个绝对唯一，且包含相对位置信息的向量。

> 正弦位置编码公式：
>
> 假设我们要把时间 $t$ 编码成一个长度为 $d$ 的向量（比如 $d=256$），向量中的每一个位置（索引标记为 $i$）的值是这样计算的：
>
> 对于向量里的偶数位置（$0, 2, 4 \dots$）：$$PE_{(t, 2i)} = \sin\left(\frac{t}{10000^{2i/d}}\right)$$
>
> 对于向量里的奇数位置（$1, 3, 5 \dots$）：$$PE_{(t, 2i+1)} = \cos\left(\frac{t}{10000^{2i/d}}\right)$$

- 注入机制：将上一步生成的位置向量经过线性层，映射到与当前U-Net 残差块通道数相同的维度（比如当前特征图是 $C \times H \times W$，时间向量就被映射成长度为 $C$ 的向量）。随后，利用广播机制，这个一维的时间向量会在空间维度（高和宽）上被复制拉伸，变成 $(C, H, W)$。然后直接与特征图相加，让图像的每个像素都带上时间戳。

## 全局视野：自注意力机制

为了让网络在预测局部噪声时能参考整张图片的全局上下文，作者在 $16 \times 16$ 的特征图分辨率层之间，加入了自注意力块。这大大提升了模型处理复杂长距离结构的能力。

在 DDPM 中，加入自注意力机制的方式简单粗暴，采取了串联的方式：

1. 特征重塑：假设卷积层输出的特征图是 $C \times H \times W$。为了送进自注意力层，我们需要把它展平。我们会把空间维度合并，变成一个序列：形状变为 $C \times N$（其中 $N = H \times W$ 就是序列长度，也就是 Token 的数量）。
2. 注意力交互：此时，每一个像素点（共 $N$ 个）就相当于 NLP 里的一个 Token，它的特征维度是 $C$。自注意力机制会计算这 $N$ 个像素点彼此之间的相似度权重，然后更新每个像素的特征。DDPM 特意选择在特征图被压缩到 $16 \times 16$ 时计算自注意力 。因为此时 $N = 256$，计算量不大，又能完美捕捉全局结构。
3. 还原形状：注意力计算完成后，序列会被重新“折叠”回 $C \times H \times W$ 的形状，然后继续送入下一个普通的卷积层中。

## 组归一化

生成模型参数量大，Batch Size 通常很小，传统的 Batch Normalization 算出的均值和方差会剧烈抖动，导致训练不稳定。

作者明确指出，他们在整个网络中统一使用了组归一化（Group Normalization）。

这是一个针对单张图片内部的通道的归一化。假设当前特征图有 256 个通道。GN 会把这 256 个通道分成若干个“组”（Groups，比如分成 32 组，每组 8 个通道）。然后，在单张图片的每一组内部，跨越这 8 个通道和空间维度去求均值和方差，并进行归一化。

----

# 实验与模型评估

## 图像生成领域的实验指标

### FID

FID 是目前图像生成领域最具权威性的图像质量评估指标。论文中特别提到，他们的无条件模型在 CIFAR10 上达到了 SOTA 的 3.17 FID 分数。

它提取真实图像和生成图片在一个预训练好的图像分类网络（通常是 Inception-V3）中的特征向量，计算真实图片特征分布和生成图片特征分布之间的距离。

FID 越低，说明生成的图片不仅清晰真实，且整体特征分布越接近真实的训练集数据。

#### 计算公式

首先，把真实图片和生成图片分别喂进图像分类网络（通常是 Inception-V3），然后分布提取 Inception 网络倒数第二层的特征向量，得到两组庞大的特征向量集合。

我们假设这两组特征在隐空间里都服从多元高斯分布。FID 计算的就是真实特征分布 $\mathcal{N}(\mu_r,\Sigma_r)$  和生成特征分布 $\mathcal{N}(\mu_g, \Sigma_g)$ 之间的 Wasserstein-2 距离：

$$\text{FID} = ||\mu_r - \mu_g||^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})$$

- $\mu_r, \mu_g$：分别是真实图片特征和生成图片特征的均值向量。
- $\Sigma_r, \Sigma_g$：分别是两者的协方差矩阵。
- $\text{Tr}$：矩阵的迹（主对角线元素之和）。

第一项算的是两个特征分布中心的距离，第二项算的是两个分布形状的重合度。两者越接近，FID 越小，说明生成的图像在高级语义特征上越接近真实世界。

#### 实验中计算

1. 大规模采样：模型训练完成之后，在推理阶段，模型随机采样生成一大批图片。在论文中，他们生成了50000 张样本。
2. 提取特征： 把这 50000 张生成的假图，以及 50000 张训练集里的真图，全部喂给一个预训练好的 Inception 网络，然后提取网络深层的特征向量。
3. 计算分布距离：利用 FID 的[[《Denoising Diffusion Probabilistic Models》论文精读#计算公式]] ，得到 FID 分数。
4. 得出结论：如果生成的 50000 张图在宏观上涵盖了各种猫、狗、飞机的特征，且特征的统计规律和真实的 50000 张图几乎重合，FID 就会非常低。我们根本不需要关心某一张生成的猫对应真实世界里的哪只猫。

> 为什么无条件生成的图片，可以与训练集作比较计算 FID？
>
> 因为模型学到的是训练集的数据分布 $p(x)$。当 DDPM 的 U-Net 拿着纯噪声开始反向去噪时，因为这个网络 $p_\theta(x_{t-1}|x_t)$ 是在 CIFAR10 上日日夜夜训练出来的，它会本能地把那些杂乱无章的噪点，往它最熟悉的那 10 个类别的特征上“雕刻” 。这就是生成模型最奇妙的地方：它的自由是有限度的自由，它的潜意识完全由它的训练集决定。

### IS

IS 是早期 GAN 时代非常流行的指标，同样使用预训练的 Inception 网络。论文中他们在 CIFAR10 上的 IS 达到了 9.46。

它重点看两点：一是清晰度（分类模型能确信图片属于某个特定类别），二是多样性（生成模型生成的图片包含各种不同的类别，而不是陷入模式崩溃，只生成同一种图）。

IS 越高越好。但因为 IS 并不和真实图片的实际数据分布直接对比，现在它的核心参考地位略逊于 FID。

#### 计算公式

IS 的公式基于 KL 散度[[《Denoising Diffusion Probabilistic Models》论文精读#KL散度]] :

$$\text{IS} = \exp \left( \mathbb{E}_{x \sim p_g} [D_{KL}(p(y|x) || p(y))] \right)$$

- $x \sim p_g$：表示从你生成的图像分布中采样出一张图片 $x$。
- $p(y|x)$：把生成的图片 $x$ 喂给 Inception 网络，网络输出的分类概率分布（清晰度）。如果图片很清晰，比如是一只狗，那它在“狗”这个类别的概率会极高，其他类别极低，这个分布会很“尖锐”。
- $p(y)$：所有生成图片经过分类器后的边缘概率分布（多样性）。如果你的模型生成的图片什么都有（猫、狗、车），那总的类别分布就会很平缓（接近均匀分布）。

我们希望单张图片特征明确（$p(y|x)$ 尖锐），而整体生成的类别丰富（$p(y)$ 平缓）。这两个分布差异越大，KL 散度越大，IS 分数就越高。

#### 实验中计算

与 FID 相同，也是利用训练好的模型生成大批样本，然后喂给分类模型，计算 IS。

### NLL / Bits/dim

NLL 主要用来衡量基于似然函数的模型（比如 VA、Flow、扩散模型）对数据分布拟合的好坏。在论文中，单位是 bits/dim。

NLL 越低越好。它类似于计算数据的无损压缩编码长度。虽然 DDPM 生成的图片 FID 表现极佳，但论文也非常坦诚地指出，他们在 NLL 指标上并没有优于其他的似然模型。

论文的精妙之处在于，他们发现虽然 DDPM 算出来的无损编码长度（NLL）比不上一些其他模型，但它的生成质量极高。这是因为模型把大把的 Bit 都消耗在了去描述那些肉眼根本察觉不到的微小失真细节上了 。

#### 计算公式

1. 似然 (Likelihood) $p_\theta(x_0)$：模型认为产生某张真实图片 $x_0$ 的概率。
2. 负对数似然 (NLL) $-\log p_\theta(x_0)$：根据香农的理论，这恰恰是用模型的逻辑去压缩这张真实图片，所需要花费的二进制比特数（Bits）。
3. 计算整个数据集的期望 $\mathbb{E}[-\log p_\theta(x_0)]$，即：用这个模型来做无损压缩，平均每张图需要多少个 Bit。
4. 计算 bits/dim：为了让不同分辨率的图片可以相互比较，研究者会把算出来的总 Bit 数，除以图片的维度总数（Dim）。

#### 实验中计算

NLL 虽然与损失函数相关，但是并不是训练过程的额外产物。在这篇论文中，NLL的计算是需要额外花时间单独去跑的。

**原因**：

- 训练时使用的是简化的目标函数$L_{simple}$[[《Denoising Diffusion Probabilistic Models》论文精读#推导简化目标函数]]
- 算 NLL 必须用真正的原版公式：既然训练时用的是为了提升图像质量而简化过的$L_{simple}$ ，那训练日志里打印出来的 Loss 值，就已经不能代表严格的 bit 了。

因此，为了算出严谨的 NLL 指标，作者必须在训练结束后，用完整的真实变分下界公式重新计算得分。不仅如此，正规的 NLL 通常要在测试集上计算。在论文表 1 中，作者明确列出了在训练集和测试集上不同的 NLL 分数，测试集和训练集的差距最多只有 0.03 bits/dim，以此来证明模型并没有过拟合。

**计算流程**：

1. 核心公式：真实变分下界（公式 5）：

   $$L = \mathbb{E}_q \left[ \underbrace{D_{KL}(q(x_T|x_0) || p(x_T))}_{L_T} + \sum_{t>1} \underbrace{D_{KL}(q(x_{t-1}|x_t, x_0) || p_\theta(x_{t-1}|x_t))}_{L_{t-1}} - \underbrace{\log p_\theta(x_0|x_1)}_{L_0} \right]$$

   在计算 NLL 时，我们实际上就是在计算并累加这三大项。

2. 逐项计算细节与物理意义：

	先验匹配项：$L_T$：

     - 公式：$D_{KL}(q(x_T|x_0) || p(x_T))$
     - 含义：它比较的是经过 $T$ 步彻底加噪后的真实图片分布 $q(x_T|x_0)$，和我们设定的纯标准正态分布 $p(x_T) = \mathcal{N}(0,I)$ 之间有多大差距 。
     - 计算细节：因为 DDPM 的前向加噪过程的方差 $\beta_t$ 是固定不变的常数，这就导致前向过程 $q$ 没有任何可学习的参数 。所以，$L_T$ 其实是一个固定的常数。在论文的实验中，这个值非常小，大约只有 $10^{-5}$ bits/dim ，计算时直接代入高斯分布的 KL 散度闭式解即可。

    核心去噪匹配项：$L_{t-1}$ （对于 $t > 1$ ）：

	- 公式：$D_{KL}(q(x_{t-1}|x_t, x_0) || p_\theta(x_{t-1}|x_t))$
	- 含义： 它对比的是“真实去噪路径”与“神经网络预测的去噪路径”。在论文 4.3 节的信息论视角下，这 $T-1$ 个项的加和构成了数据压缩中的“率” (Rate)。
	- 计算细节：由于条件分布 $q(x_{t-1}|x_t, x_0)$ 和神经网络的预测 $p_\theta(x_{t-1}|x_t)$ 都是高斯分布。两个高斯分布算 KL 散度是有现成数学公式的（Rao-Blackwellized 闭式表达式），完全不需要采样近似。算法会遍历时间步 $t$，算出所有的 $L_{t-1}$ 并求和。

	重构/解码项：$L_0$：

	- 公式：$-\log p_\theta(x_0|x_1)$
	- 含义：在最后一步，我们要从连续的高斯分布 $x_1$ 生成离散的真实像素 $x_0$（像素值是 0 到 255 的整数，论文中线性缩放到了 [-1, 1] 区间）。这一项评估的是最终生成的图像和真实图像有多像。在信息论视角下，它代表了压缩过程中的“失真” (Distortion)。
	- 计算细节：为了得到严谨的离散对数似然，作者使用了一个基于高斯分布推导出的独立离散解码器（公式 13）。它通过计算高斯概率密度函数在像素区间内的积分，来得到精确的概率值 。这样就能确保算出来的变分下界，是一个真实离散数据的无损编码长度，不需要给数据人为添加噪声 。
3. NLL 的最终汇总与单位换算：

   当上述 $L_T$、$\sum L_{t-1}$ 以及 $L_0$ 计算完毕后，我们就得到了变分下界 $L$ 的总数值。但由于计算机在算对数时默认用的是自然对数（底数为 $e$），算出来的单位叫 Nats。将总 Nats 值除以 $\ln(2)$，转换为以 2 为底的比特数 (Bits)。再将得到的总 Bits 除以图像的维度总数 $D$ （对于 CIFAR10，$D = 32 \times 32 \times 3 = 3072$）。得到的值就是论文表 1 中报告的 NLL 值，例如 3.17 bits/dim 。

## 实验与超参数设置

### 数据集与预处理

为了公平对比，研究者都会在公开数据集上测试。这篇论文中主要用到了：

- CIFAR10：32x32 的低分辨率小图，包含 10 个类别。虽然图小，但极度考验模型的基础生成能力。标准划分是 50,000 张训练集图片和 10,000 张测试集图片。
- LSUN & CelebA-HQ：分辨率达到了 256x256 。LSUN 包含各种特定场景（如教堂、卧室），CelebA-HQ 则是高清人脸数据集。能够在这些数据集上生成清晰的高清图，说明模型具备很强的实用价值。

预处理：真实的图像数据（像素值原本是 0 到 255 的整数）会被线性缩放到 $[-1, 1]$ 的区间 。这样做是为了保证神经网络在处理初始的标准正态先验分布 $p(x_T)$ 时，输入数据的尺度是一致的 。

### 前向过程参数调度

- 时间步长 $T$ ：作者将总的扩散时间步设为了 $T = 1000$ 。
- 方差调度表：前向过程的方差 $\beta_t$ 是固定常数，并没有让网络去学习 。作者采用了一种线性递增的策略，$\beta_t$ 从 $\beta_1 = 10^{-4}$ 线性增加到 $\beta_T = 0.02$ 。

> 为什么这么设？
>
> 保持这些常数相对较小，可以确保前向加噪和反向去噪过程具有近似相同的函数形式 。同时，经过 1000 步加噪后，到了 $x_T$ 时，图像的信噪比已经极小，原本的信号被彻底破坏，几乎等同于纯粹的高斯噪声。

### 训练与优化技巧

- 优化器与 EMA：使用了 Adam 优化器。在保存模型参数时，作者使用了指数移动平均（EMA），衰减率设为 0.9999。
- Dropout：在训练 CIFAR10 数据集时，作者将 Dropout 率设为 0.1。如果不加 Dropout，生成的样本质量会变差，出现过拟合伪影。

> EMA：
>
> EMA 是一种在深度学习训练时非常常用的参数平滑技巧。
>
> 工作逻辑：
>
> 在训练神经网络时，模型有一组真正参与梯度反向传播和更新的“当前参数”（记作 $\theta_{\text{current}}$）。 当开启 EMA 时，程序会在后台悄悄维护另一组“影子参数”（记作 $\theta_{\text{EMA}}$）。每次当前参数更新后，影子参数会按照下面的公式进行微调：$$\theta_{\text{EMA}} = \beta \cdot \theta_{\text{EMA}} + (1 - \beta) \cdot \theta_{\text{current}}$$
>
> - $\beta$ 就是衰减因子，DDPM 里设置得非常接近 1（$\beta = 0.9999$）
> - 这意味着，影子参数每次只吸收当前参数极小一部分（$0.01\%$）的新变化，保留了绝大部分（$99.99\%$）的自身历史记忆。
>
> EMA 的核心作用：
>
> - 稳定训练过程：在训练后期，或者面对像生成模型这样极其复杂的损失函数空间时，每个 Batch 计算出来的梯度可能会非常嘈杂，导致模型的参数在最优解附近剧烈震荡。EMA就像一个减震器，它记录了参数历史移动的平滑轨迹，过滤掉了短期的剧烈抖动。
> - 提高生成质量：可以把 EMA 理解为在时间维度做了一次模型集成。它融合了模型在过去成千上万次迭代里的状态。在图像生成领域，经验表明，这种平滑融合后的权重往往能生成质量更高、更锐利的图像。
> - 训练与推理分离：EMA 参数是不参与训练时的反向传播的。在训练时不断更新 $\theta_{\text{current}}$，而当训练结束，准备去算 FID 分数或者展示生成的图片时，我们会丢弃 $\theta_{\text{current}}$，加载平滑且稳定的 $\theta_{\text{EMA}}$ 来进行推理采样。

## 消融实验

### 均值 $\tilde{\mu}$ vs 噪声 $\epsilon$

- 配方 A：预测真实的后验均值 $\tilde{\mu}$ (Baseline)：

  作者发现，预测均值只有在配合极其严格的“真实变分下界 ($L$)”目标函数时才勉强有效（FID 约为 23.69） 。一旦换成简单的均方误差目标函数，训练就不稳定了，生成的图片惨不忍睹 。

- 配方 B：预测加入的噪声 $\epsilon$ (Ours / DDPM 的最终选择)：

  表现极佳！尤其是在配合简化版目标函数时，FID 瞬间飙升到了 3.17 的 SOTA 级别 。

  **深层原因**：作者在 3.2 节用数学证明了，预测噪声 $\epsilon$ 在数学形式上不仅等价于朗之万动力学 (Langevin dynamics)，而且让目标函数看起来非常像“去噪得分匹配 (denoising score matching)” 。这赋予了模型极好的物理和概率学意义。

### 真实变分下界 $L$ vs 简化均方误差 $L_{simple}$

- 配方 A：使用真实的变分下界 ($L$)
    - 这是严谨推导出来的公式（也就是我们在前面算 NLL 用的那个）。它对每一个时间步 $t$ 的 Loss 都分配了一个极其复杂的权重系数。
    - **缺点**：在真实的 $L$ 中，时间步 $t$ 很小（也就是快要生成最终清晰图像、噪声极小）的时候，权重特别大。这就导致网络把大部分精力都花在了去抠那些极其微小的噪点上。
- 配方 B：使用简化版目标函数 ($L_{simple}$)
    - 作者大笔一挥，直接把前面那一坨复杂的权重系数全扔了，变成了最纯粹的非加权均方误差。$$L_{simple} = \mathbb{E}_{t, x_0, \epsilon} [ \| \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t) \|^2 ]$$
    - 作者发现，丢掉权重相当于降低了小时间步（$t$ 很小）的损失权重 。这样一来，网络就不会在最后那些鸡毛蒜皮的微小噪点上死磕，而是强迫自己把更多的精力放在 $t$ 较大时（也就是满屏雪花点时）的高难度去噪任务上 。
    - 结果：虽然这样算出来的 NLL变差了，但生成的图像质量（FID）实现了质的飞跃 ！这就是典型的“为了好看，牺牲了理论上的严谨压缩率”。

### 方差 $\Sigma_\theta$ 的设定

- 配方 A：让网络去学习方差
    - 结果：作者在表 2 中报告，如果把方差也放进网络里去学习，会导致训练极其不稳定，并且生成的图片质量很差 。
- 配方 B：人工固定方差为常数 (Fixed isotropic $\Sigma$)
    - 作者直接把方差固定为和前向加噪时有关的常数（比如 $\beta_t$） 。
    - 结果：在预测噪声 $\epsilon$ 的前提下，固定方差不仅让网络更容易训练，而且配合 $L_{simple}$ 拿到了全场最高的生成质量 。

### 最终配置

经过一系列消融实验，作者敲定了最终的模型设定：

使用 U-Net 预测噪声 $\epsilon$ + 使用不带权重的简化的均方误差 $L_{simple}$ 作为 Loss + 人工固定方差。

这套组合拳奠定了后续几乎所有主流扩散模型（包括 Stable Diffusion 的底层逻辑）的基础。

## DDPM 的高阶特性

### 渐进式生成与有损压缩

在传统的 GAN（生成对抗网络）中，图像通常是“一瞬间”生成的，我们要么得到一张好图，要么得到一张崩坏的图。但扩散模型是一个 $T=1000$ 步的漫长过程，这就带来了一个极其迷人的特性：渐进式生成。

观察论文中的图 6 ，会发现在反向去噪的早期（$t$ 比较大的时候，比如从 1000 到 500），模型主要在确定图像的大尺度特征，比如天空的位置、鸟的轮廓 。而到了晚期（$t$ 接近 0 的时候），模型才开始填充毛发、纹理等极其微小的细节。

作者绘制了失真率-比特率曲线（图 5）。他们发现，在这个 1000 步的降噪旅程中，刚开始消耗极少的比特（Bits）就能让图像的均方误差（失真）大幅下降 。这说明大部分的“信息编码量”其实都被模型用在了最后那些人类肉眼几乎无法察觉的微小像素波动上了 。这也呼应了我们之前讨论的，为什么 DDPM 的 NLL 分数一般，但看起来极度逼真。

作者甚至提出，高斯扩散模型可以被解释为一种具有广义比特排序的自回归模型，这种排序方式是普通的重排像素坐标无法表达的 。

> **重排像素坐标**：
>
> 传统的生成顺序：自回归模型生成图片，就像我们平时写字或者打印机工作一样，是逐个像素（Pixel-by-Pixel）生成的。它通常遵循一个固定的空间坐标顺序：从左上角第一个像素开始，然后第二个，一直到右下角最后一个。
>
> 重排像素坐标：有些研究者发现，从左上角按顺序生成未必是最好的。于是他们尝试“重排”这个顺序，比如：先生成所有偶数坐标的像素，再生成奇数坐标的像素；或者从图像正中心开始，像螺旋一样往外生成。
>
> 无论怎么“重排像素坐标”，它本质上依然是在物理空间上决定“先画哪一个点，再画哪一个点”。
>
> **广义比特排序的自回归模型**：
>
> DDPM 在生成图片时，根本不是逐个像素生成的，它是对着整张图片的所有像素同时开工。那为什么作者还说它是一种“自回归模型”呢？
>
> 作者在 4.3 节提出了一个极其精妙的思想实验 ： 假设一张图片有 3072 个像素（维度），如果我们把扩散步数 $T$ 也设为 3072，并且规定每一步的“加噪”不是加高斯噪声，而是精确地抹掉（Mask out）一个像素。那么，反向去噪的过程，就变成了每次精确地预测并填补一个被抹掉的像素。在这个特定的极端设定下，扩散模型就完美等价于一个传统的自回归模型了 ！
>
> 但是，真实的 DDPM 并没有使用这种“每次抹掉一个像素”的做法：
>
> - 广义的比特（信息）排序： DDPM 使用的是连续的高斯噪声，在 1000 步里对着所有像素同时加噪或去噪 。
>
>   - 这就导致 DDPM 的生成顺序不再是物理空间上的“先画左边，再画右边”，而是信息维度（频率）上的“先画大轮廓，再画小细节”。
>   - 在信息论里，一张图片所包含的信息是由一段段二进制比特（Bits）组成的。DDPM 等于是在解码时，先解码描述全局结构的比特，再解码描述微小失真的比特。
>
>    结论： 作者说这是一种“广义比特排序（Generalized bit ordering）” ，是因为这种从“模糊到清晰”、“从低频到高频”的信息逐步释放过程，是在物理空间上无论怎么“重排像素坐标”都绝对无法模拟出来的 。作者认为，给图像全局加上高斯噪声，可能比单独抹掉某些像素显得更自然，这也就是为什么 DDPM 能生成如此逼真图像的一个重要潜在原因（归纳偏置 Inductive bias） 。

### 隐空间平滑插值

在图像生成中，我们经常想把两张图片混合在一起（比如把一张男性的脸平滑过渡到一张女性的脸）。如果你直接在像素层面把两张图按比例相加，只会得到一张带有重影的“幽灵图”。DDPM 提供了一种优雅的隐空间插值方案（见图 8 和图 9）。

- 插值流程：
    1. 随机编码（加噪）：拿到两张真实的源图像 $x_0$ 和 $x_0'$，利用前向过程把它们加噪到同一个中间时间步 $t$，得到隐变量 $x_t$ 和 $x_t'$ 。
    2. 线性混合：在这个全是噪点的隐空间里，把两个特征按比例 $\lambda$ 混合：$\bar{x}_t = (1-\lambda)x_t + \lambda x_t'$ 。
    3. 反向解码（去噪）：把混合后的 $\bar{x}_t$ 喂给训练好的 U-Net，让它一路去噪回到 $\bar{x}_0$ 。
- 为什么这样做效果好？：

  混合后的噪点图本来是不符合真实自然图像规律的（它脱离了图像流形）。但是，DDPM 的反向去噪过程本质上是一个“去除伪影”的过滤器 。它能硬生生地把这个不自然的混合态，强行拉回到真实图像的数据分布中，从而生成出极其合理、平滑的高质量重构图像 。

- 时间步 $t$ 的控制魔力：
    - 如果 $t$ 比较小，模型只会保留大体框架，插值时只改变微小的细节 。
    - 如果在 $t=500$ 左右插值，生成的图像能够平滑地改变姿态、肤色、发型、表情和背景等高级属性 。
    - 如果在最极端的 $t=1000$ 步进行混合，原本两张图的信息已经全部丢失了，这时的插值实际上就是在两个纯噪声之间过渡，最终会生成完全新颖的随机样本 。

----

# 相关工作与理论联系

## 变分自编码器（VAE）与归一化流（Normalizing Flows）

在 DDPM 出现之前，VAE 和 Flow 是两支非常强大的基于隐变量的生成模型。VAE 通过学习数据的均值和方差来编码，Flow 则是通过一系列可逆的数学变化，把复杂数据强行映射成简单的高斯分布。

作者指出，虽然 DDPM 看起来很像它们（都是从高斯分布变回图像），但 DDPM 的前向加噪过程 $q$ 是完全人为固定、没有任何参数的 。而且到了最后一步 $x_T$，图像和原始数据 $x_0$ 之间的互信息几乎降到了零 。这使得 DDPM 训练起来比 VAE 更稳定，结构比 Flow 更自由。

> 互信息
>
> 互信息衡量的是“知道变量 $Y$ 的值，能减少多少关于变量 $X$ 的不确定性”。如果 $X$ 和 $Y$ 毫无关系，互信息就是 0；如果 $Y$ 就是 $X$，互信息就是 $X$ 本身的信息量。
>
> 计算公式：
>
> 假设有两个随机变量 $X$ 和 $Y$：$$I(X; Y) = H(X) - H(X|Y)$$
>
> - $H(X)$ 是 $X$ 原本的熵（原本的不确定性）。
> - $H(X|Y)$ 是在已知 $Y$ 的情况下，对 $X$ 剩下的条件熵（剩下的不确定性）。
> - 两者一减，就是 $Y$ 消除掉的不确定性，也就是它们共享的信息量
>
> 如果用概率分布展开写（连续变量用积分，离散变量用求和），公式是：$$I(X; Y) = \iint p(x, y) \log \frac{p(x, y)}{p(x)p(y)} dx dy$$如果是独立的，联合概率 $p(x,y)$ 就等于边缘概率的乘积 $p(x)p(y)$，这时候 $\log(1) = 0$，互信息为 0。
>
> 对于 DDPM，“顶层的隐变量 $x_T$ 与真实数据 $x_0$ 的互信息几乎为零“。即原图的信息已经被彻底摧毁了，它们之间共享的信息量 $I(x_0; x_T) \approx 0$

### VAE

变分自编码器（VAE），是一个自编码器（Autoencoder），分为编码器和解码器。普通的自编码器会把一种图片压缩成一个固定向量，然后再还原。但 VAE 认为：固定的向量无法用来”生成“新图片。所以，VAE 的 Encoder 不输出固定的向量，而是输出一个高斯分布的均值 $\mu$ 和方差 $\sigma$ 。

#### 生成过程

1. 编码器压缩：

   输入一张 $3 \times 32 \times 32$ 的图片，经过一层层的卷积层（下采样），图片的长宽越来越小，通道数越来越深。到了最后，把它展平（Flatten），变成一个一维的长向量（比如长度为 512 的向量）。这个向量浓缩了整张图片的特征。

2. 计算 $\mu$ 和 $\sigma$ ：

   拿到这个 512 维的向量后，分别通过两个独立的全连接层： 

   第一个全连接层：负责输出一个长度为 $d$（比如 128）的向量，这个向量就是均值 $\mu$。

   第二个全连接层：负责输出另一个长度为 $d$（128）的向量，这个向量代表方差的对数 $\log(\sigma^2)$。（_注：工程上通常让网络输出方差的对数而不是直接输出 $\sigma$，是因为神经网络可能输出负数，而方差必须是正数。加上指数函数就能完美解决这个问题_）。

3. 重参数化采样：

   现在我们有了 $\mu$ 和 $\sigma$。我们从标准正态分布 $\mathcal{N}(0, I)$ 中随机抽一个噪声 $\epsilon$。 然后代入公式：$z = \mu + \sigma \odot \epsilon$。 这样我们就得到了长度为 128 的隐变量 $z$。

4. 解码器放大：

   拿着这个 128 维的隐变量 $z$，通过一个全连接层把它升维，然后再通过一层层的转置卷积（反卷积 / 上采样），一步步把它放大回 $3 \times 32 \times 32$ 的尺寸，变成最终生成的图片 $\hat{x}$。

#### 与DDPM 的联系

- 相似点：都要算 ELBO（变分下界），都用了重参数化技巧。
- 不同点：VAE 的编码器是需要用神经网络去学习的，而 DDPM 的“编码器”（也就是前向加噪过程 $q$）是完全人为写死、不需要训练的固定公式；VAE 只在最中间的极度压缩的瓶颈处进行一次重参数化采样，而 DDPM 则是在反向去噪过程中进行 $T$ 次重参数化采样。

### Normalizing Flows

Normalizing Flows 的核心思想是：如果在数学建模中遇到一个极其复杂的概率分布，我们很难直接用公式写出来。Flow 的思路就是，先找一个最简单的标准高斯分布 $\mathcal{N}(0,I)$ ，然后寻找一系列极其复杂的数学函数，把这个简单的高斯分布”扭曲、拉伸、变换“成那个复杂的真实分布。

**苛刻的数学要求**：为了保证变换之后还能算得出概率（即算得出积分），Flow 要求它使用的每一个变换函数必须满足两个条件：

1. 必须是双射（即可逆）：能从噪声变成图，也能同一个公式反推从图变成噪声。
2. 必须容易计算雅可比行列式：用来追踪空间在扭曲时体积的变化率。

#### 与DPPM的联系

- 相似点：都是把高斯噪声映射成复杂图像。
- 不同点：Flow 因为要严格满足“可逆”和“算雅可比矩阵”的数学限制，导致它的神经网络架构设计得极其痛苦和受限（必须设计特定的可逆模块）。而 DDPM 放弃了严格的可逆函数，转而用马尔可夫链和高斯转移概率来逼近，这让 DDPM 可以随心所欲地使用 U-Net 这种超级强大的非线性网络。

## 基于能量的模型（EBM）与得分匹配（Score Matching）

能量模型：物理学中，系统总是倾向于处于能量最低的状态。EBM 把一幅逼真的图像看作是”低能量“状态，一堆乱码噪点看作是”高能量“状态。

得分：在概率论中，数据分布对数的梯度$\nabla_x \log p(x)$ 被称为”得分“。它本质上是一个向量场，所有的箭头都指向数据概率密度最高（也就是最真实、能量最低）的地方。

### 与 DDPM 的联系

作者在论文中证明，我们用来预测噪声的 U-Net，在数学本质上，完全等价于多尺度的去噪得分匹配。换句话说，U-Net 预测的噪声方向，其实就是引导图像走向更高真实度的”得分向量“的反方向。

## 退火朗之万动力学

退火朗之万动力学是统计物理学里的一个概念。我们在寻找最真实的图像状态的过程中，如果一直按照梯度下降的方向走，很容易陷入局部最优。朗之万动力学的策略是：一边沿着梯度下降的方向走，一边加入随机高斯噪声。

### 与 DDPM 的联系

作者指出，DDPM 算法 2 中的采样过程，完美对应了退火朗之万动力学的采样过程。更值得一提的是，传统得分匹配的朗之万采样参数都是人为瞎凑的，而 DDPM 通过变分推断，用严谨的数学推导出了这些采样系数的最优解。


---

# 总结与广泛影响

## 核心贡献总结

- 理论上的大一统：DDPM 将变分推断、马尔可夫链、去噪得分匹配、退火朗之万动力学、自回归模型以及渐进式有损压缩等多个看似不相关的领域联系在一起。
- 工程上的突破：其在无条件图像生成上取得了非常好的效果，证明了扩散模型在图像数据上拥有极佳的归一化偏置。

## 深度生成模型的广泛社会影响

- 潜在风险：可以被恶意用来生成虚假的政治人物图像和视频。此外，生成模型会吸收并放大训练数据集里潜藏的偏见。
- 未来发展潜力：扩散模型不仅在艺术创作上有潜力，在数据压缩和无监督表示学习上也大有可为。

---
